{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==================================================================================\n# TITAN VALIDATION FRAMEWORK (TVF) â€” OMNI EDITION (FINAL)\n# Role: Senior Data Architect\n# Capabilities: \n#   1. DE Ops (Schema, Volume, Dupes)      2. Statistical (Num/Cat Drift)\n#   3. Structural (PCA, Manifold)          4. Causal (Stability, Paradox)\n#   5. Logic/Business Rules                6. Forensics (Benford)\n#   7. Ethics (Fairness/Bias)              8. Security (Leakage)\n# ==================================================================================\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np\nimport pandas as pd\nimport logging\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Tuple\nfrom scipy.stats import ks_2samp, chisquare\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n# -----------------------------\n# 1. CONFIGURATION\n# -----------------------------\n@dataclass\nclass TVFConfig:\n    # Statistical\n    drift_alpha: float = 0.05\n    reconstruction_error_threshold: float = 0.05 \n    min_explained_variance_ratio: float = 0.90 \n    \n    # Integrity & Ops\n    max_null_spike: float = 0.10\n    min_numeric_coercion: float = 0.98\n    max_volumetric_drift: float = 0.50  # Alert if row count changes by +/- 50%\n    max_duplicate_rate: float = 0.0     # Zero tolerance for dupes\n    \n    # Forensics\n    enable_benfords_law: bool = True\n    \n    # Causal & Ethics\n    max_correlation_drift: float = 0.25\n    simpsons_threshold: float = 0.3\n    max_fairness_disparity: float = 0.25 # Max 25% diff in target mean between groups\n    \n    # Compute\n    max_cardinality: int = 100\n    allow_zero_variance: bool = False\n    max_leakage_r2: float = 0.98\n    \n    # Temporal\n    max_recency_days: int = 30 \n    \n    # Unsupervised\n    enable_iforest: bool = True\n    iforest_contamination: float = 0.02\n\n# -----------------------------\n# 2. THE ENGINE\n# -----------------------------\nclass TitanValidationFramework:\n    def __init__(self, reference_data: pd.DataFrame, config: TVFConfig = None):\n        self.logger = logging.getLogger(\"TVF\")\n        if not self.logger.handlers:\n            self.logger.setLevel(logging.INFO)\n            self.logger.addHandler(logging.StreamHandler())\n            \n        self.config = config or TVFConfig()\n        self.reference = reference_data.copy()\n        \n        # Learn Baseline\n        self.num_cols = self.reference.select_dtypes(include=[np.number]).columns.tolist()\n        self.cat_cols = self.reference.select_dtypes(include=['object', 'category']).columns.tolist()\n        self.ref_corr = self.reference[self.num_cols].corr()\n        self.ref_nulls = self.reference.isna().mean()\n        self.ref_count = len(self.reference)\n        \n        self.ref_cat_freqs = {c: self.reference[c].value_counts(normalize=True) for c in self.cat_cols}\n        \n        # Train Models\n        self._train_unsupervised()\n        self._train_structure()\n        \n        self.logger.info(f\"TVF Omni Online. Baseline: {self.ref_count} rows.\")\n\n    def _train_unsupervised(self):\n        self.iforest = None\n        if self.config.enable_iforest and len(self.reference) > 50:\n            self.iforest = IsolationForest(contamination=self.config.iforest_contamination, random_state=42, n_jobs=-1)\n            self.iforest.fit(self.reference[self.num_cols].fillna(0))\n\n    def _train_structure(self):\n        self.scaler = StandardScaler()\n        X = self.reference[self.num_cols].fillna(0)\n        X_scaled = self.scaler.fit_transform(X)\n        self.pca = PCA(n_components=0.95)\n        self.pca.fit(X_scaled)\n        \n        X_recon = self.pca.inverse_transform(self.pca.transform(X_scaled))\n        self.ref_recon_error = np.mean(np.square(X_scaled - X_recon))\n        self.ref_explained_var = np.sum(np.var(self.pca.transform(X_scaled), axis=0)) / np.sum(np.var(X_scaled, axis=0))\n\n    def validate(self, new_data: pd.DataFrame, target_col: str = None, date_col: str = None, \n                 consistency_rules: List[str] = None, subgroups: List[str] = None) -> Tuple[bool, Dict]:\n        \n        report = {\"modules\": {}, \"traffic_light\": None}\n        \n        # --- SCANS ---\n        report[\"modules\"][\"integrity\"] = self._scan_integrity(new_data)\n        report[\"modules\"][\"ops\"] = self._scan_ops(new_data)\n        report[\"modules\"][\"drift_num\"] = self._scan_drift_numeric(new_data)\n        report[\"modules\"][\"drift_cat\"] = self._scan_drift_categorical(new_data)\n        report[\"modules\"][\"forensics\"] = self._scan_forensics(new_data)\n        report[\"modules\"][\"structure\"] = self._scan_structure(new_data)\n        report[\"modules\"][\"stability\"] = self._scan_stability(new_data)\n        report[\"modules\"][\"paradox\"] = self._scan_simpsons(new_data, target_col, subgroups)\n        report[\"modules\"][\"fairness\"] = self._scan_fairness(new_data, target_col, subgroups)\n        report[\"modules\"][\"logic\"] = self._scan_logic(new_data, consistency_rules)\n        report[\"modules\"][\"temporal\"] = self._scan_temporal(new_data, date_col)\n        report[\"modules\"][\"compute\"] = self._scan_compute(new_data)\n        report[\"modules\"][\"leakage\"] = self._scan_leakage(new_data, target_col)\n        report[\"modules\"][\"anomalies\"] = self._scan_anomalies(new_data)\n\n        # Verdict\n        report[\"traffic_light\"] = self._generate_report(report)\n        failed = len(report[\"traffic_light\"][report[\"traffic_light\"][\"Status\"] == \"RED\"]) > 0\n        return (not failed), report\n\n    # --- SCANNERS ---\n    def _scan_ops(self, df):\n        issues = []\n        ratio = len(df) / (self.ref_count + 1e-9)\n        if abs(1 - ratio) > self.config.max_volumetric_drift:\n            issues.append(f\"Volumetric Drift (Size Ratio: {ratio:.2f}x)\")\n        dupe_rate = df.duplicated().mean()\n        if dupe_rate > self.config.max_duplicate_rate:\n            issues.append(f\"Duplicate Rows ({dupe_rate:.1%})\")\n        return {\"issues\": issues}\n\n    def _scan_fairness(self, df, target, groups):\n        if not target or not groups or target not in df.columns: return {\"issues\": []}\n        issues = []\n        for g in groups:\n            if g not in df.columns: continue\n            means = df.groupby(g)[target].mean()\n            if len(means) < 2: continue\n            disparity = (means.max() - means.min()) / (abs(means.min()) + 1e-9)\n            if disparity > self.config.max_fairness_disparity:\n                issues.append(f\"Fairness Warning on '{g}' (Disparity: {disparity:.1%})\")\n        return {\"issues\": issues}\n\n    def _scan_drift_categorical(self, df):\n        drifted = []\n        for c in self.cat_cols:\n            if c not in df.columns: continue\n            ref_freq = self.ref_cat_freqs.get(c)\n            new_counts = df[c].value_counts()\n            common_cats = ref_freq.index.intersection(new_counts.index)\n            if len(common_cats) < 2: continue\n            obs = new_counts[common_cats].sort_index().values\n            exp = ref_freq[common_cats].sort_index().values * len(df)\n            exp = exp * (obs.sum() / exp.sum())\n            try:\n                if chisquare(f_obs=obs, f_exp=exp)[1] < self.config.drift_alpha: \n                    drifted.append(c)\n            except: pass\n        return {\"drifted\": drifted}\n\n    def _scan_forensics(self, df):\n        if not self.config.enable_benfords_law: return {\"suspicious\": []}\n        suspicious = []\n        benford_probs = np.log10(1 + 1/np.arange(1, 10))\n        for c in self.num_cols:\n            if c not in df.columns: continue\n            try:\n                digits = df[c].astype(str).str.lstrip('-').str[0]\n                digits = digits[digits.isin([str(i) for i in range(1, 10)])].astype(int)\n                if len(digits) < 100: continue\n                counts = digits.value_counts().sort_index()\n                obs = np.array([counts.get(i, 0) for i in range(1, 10)])\n                if chisquare(obs, f_exp=benford_probs * len(digits))[1] < 0.001: \n                    suspicious.append(f\"{c} (Benford Viol.)\")\n            except: pass\n        return {\"suspicious\": suspicious}\n\n    def _scan_structure(self, df):\n        try:\n            X = self.scaler.transform(df[self.num_cols].fillna(0))\n            X_recon = self.pca.inverse_transform(self.pca.transform(X))\n            ratio = np.mean(np.square(X - X_recon)) / max(self.ref_recon_error, 1e-6)\n            var_retention = (np.sum(np.var(self.pca.transform(X), axis=0)) / np.sum(np.var(X, axis=0))) / self.ref_explained_var\n            if ratio > (1 + self.config.reconstruction_error_threshold):\n                return {\"drifted\": True, \"msg\": f\"Structure Shift (Error Ratio: {ratio:.2f}x)\"}\n            if var_retention < self.config.min_explained_variance_ratio:\n                return {\"drifted\": True, \"msg\": f\"Structure Collapse (Var Retention: {var_retention:.1%})\"}\n            return {\"drifted\": False}\n        except Exception as e: return {\"drifted\": False, \"msg\": str(e)}\n\n    def _scan_integrity(self, df):\n        issues = []\n        missing = set(self.reference.columns) - set(df.columns)\n        if missing: issues.append(f\"Missing Columns: {list(missing)[:3]}...\")\n        for c in df.columns:\n            if (df[c].isna().mean() - self.ref_nulls.get(c, 0)) > self.config.max_null_spike:\n                issues.append(f\"{c}: Null Spike\")\n        return {\"issues\": issues}\n\n    def _scan_drift_numeric(self, df):\n        drifted = []\n        for c in self.num_cols:\n            if c not in df.columns: continue\n            try:\n                if ks_2samp(self.reference[c].dropna(), df[c].dropna())[1] < self.config.drift_alpha:\n                    drifted.append(c)\n            except: pass\n        return {\"drifted\": drifted}\n\n    def _scan_stability(self, df):\n        common = [c for c in self.num_cols if c in df.columns]\n        if len(common) < 2: return {\"broken\": []}\n        diff = (self.reference[common].corr() - df[common].corr()).abs()\n        broken = []\n        for i in range(len(common)):\n            for j in range(i+1, len(common)):\n                if diff.iloc[i, j] > self.config.max_correlation_drift:\n                    broken.append(f\"{common[i]}-{common[j]}\")\n        return {\"broken\": broken}\n\n    def _scan_simpsons(self, df, target, groups):\n        reversals = []\n        if not target or not groups: return {\"reversals\": []}\n        feats = [f for f in self.num_cols if f != target and f in df.columns]\n        for g in groups:\n            if g not in df.columns: continue\n            for f in feats:\n                try:\n                    g_corr = df[f].corr(df[target])\n                    if abs(g_corr) < self.config.simpsons_threshold: continue\n                    for sub in df[g].unique():\n                        sub_df = df[df[g] == sub]\n                        if len(sub_df) < 30: continue\n                        s_corr = sub_df[f].corr(sub_df[target])\n                        if (np.sign(g_corr) != np.sign(s_corr)) and abs(s_corr) > self.config.simpsons_threshold:\n                            reversals.append(f\"{f} (Global {g_corr:.2f}, {sub} {s_corr:.2f})\")\n                            break\n                except: pass\n        return {\"reversals\": reversals}\n\n    def _scan_logic(self, df, rules):\n        violations = []\n        if rules:\n            for r in rules:\n                try:\n                    if (~df.eval(r).fillna(False)).sum() > 0: violations.append(f\"Rule Failed: {r}\")\n                except: pass\n        return {\"violations\": violations}\n\n    def _scan_temporal(self, df, date_col):\n        issues = []\n        if date_col and date_col in df.columns:\n            try:\n                dates = pd.to_datetime(df[date_col])\n                if not dates.is_monotonic_increasing: issues.append(\"Time Travel (Unsorted)\")\n                if (pd.Timestamp.now() - dates.max()).days > self.config.max_recency_days:\n                    issues.append(\"Stale Data\")\n            except: issues.append(\"Date Parse Failed\")\n        return {\"issues\": issues}\n\n    def _scan_compute(self, df):\n        issues = []\n        for c in df.select_dtypes(include=['object', 'category']):\n            if df[c].nunique() > self.config.max_cardinality: issues.append(f\"{c}: High Cardinality\")\n        if not self.config.allow_zero_variance:\n            for c in self.num_cols:\n                if c in df.columns and df[c].var() == 0: issues.append(f\"{c}: Zero Variance\")\n        return {\"issues\": issues}\n\n    def _scan_leakage(self, df, target):\n        leaks = []\n        if target and target in df.columns:\n            y = df[target].fillna(0)\n            for c in self.num_cols:\n                if c == target or c not in df.columns: continue\n                try:\n                    if LinearRegression().fit(df[[c]].fillna(0), y).score(df[[c]].fillna(0), y) > self.config.max_leakage_r2:\n                        leaks.append(c)\n                except: pass\n        return {\"leaks\": leaks}\n\n    def _scan_anomalies(self, df):\n        if not self.iforest: return {\"rate\": 0.0, \"status\": \"SKIPPED\"}\n        rate = (self.iforest.predict(df[self.num_cols].fillna(0)) == -1).mean()\n        return {\"rate\": rate, \"status\": \"RED\" if rate > self.config.iforest_contamination * 4 else \"GREEN\"}\n\n    def _generate_report(self, report):\n        rows = []\n        map_ = {\n            \"integrity\": \"Integrity\", \"ops\": \"Ops Health\", \"drift_num\": \"Numeric Drift\", \n            \"drift_cat\": \"Categorical Drift\", \"structure\": \"Multivariate Drift\", \n            \"stability\": \"Causal Stability\", \"paradox\": \"Paradox\", \"fairness\": \"Algorithmic Fairness\",\n            \"logic\": \"Business Logic\", \"temporal\": \"Temporal\", \"compute\": \"Compute\", \n            \"leakage\": \"Data Leakage\", \"anomalies\": \"Multivariate Anomaly\", \"forensics\": \"Forensics\"\n        }\n        for mod, label in map_.items():\n            data = report[\"modules\"].get(mod, {})\n            errs = []\n            for k in [\"issues\", \"drifted\", \"broken\", \"violations\", \"reversals\", \"leaks\", \"suspicious\"]:\n                if isinstance(data.get(k), list): errs.extend(data[k])\n            if mod == \"anomalies\" and data.get(\"status\") == \"RED\": errs.append(f\"Anomaly Rate {data['rate']:.1%}\")\n            if mod == \"structure\" and data.get(\"drifted\"): errs.append(data.get(\"msg\"))\n            \n            for e in errs:\n                rows.append({\"Module\": label, \"Feature\": e.split(\":\")[0] if \":\" in str(e) else \"Global\", \"Status\": \"RED\", \"Reason\": str(e)})\n                \n        return pd.DataFrame(rows).sort_values(\"Module\") if rows else pd.DataFrame([{\"Status\": \"GREEN\", \"Reason\": \"All Checks Passed\"}])\n\n# ==================================================================================\n# 3. TITAN OMNI GAUNTLET: The \"No Gaps\" Proof (CORRECTED SEQUENCE)\n# ==================================================================================\nif __name__ == \"__main__\":\n    from sklearn.datasets import load_diabetes\n    print(\"--- 1. INITIALIZING TITAN OMNI ---\")\n    raw = load_diabetes(as_frame=True, scaled=False)\n    df_ref = raw.frame\n    df_ref[\"Date\"] = pd.date_range(end=pd.Timestamp.now(), periods=len(df_ref))\n    df_ref[\"Region\"] = np.random.choice([\"A\", \"B\"], size=len(df_ref)) \n    \n    titan = TitanValidationFramework(df_ref)\n\n    # 4. GENERATE 5-PRONGED ATTACK\n    print(\"\\n--- 2. GENERATING 'HYDRA' ATTACK DATA ---\")\n    df_attack = df_ref.copy()\n    \n    # Attack 1: Volumetric (Drop 60% of data) -> Triggers >50% Threshold\n    df_attack = df_attack.sample(frac=0.4, random_state=42)\n    \n    # Attack 2: Fairness (Bias)\n    mask_a = df_attack[\"Region\"] == \"A\"\n    df_attack.loc[mask_a, \"target\"] *= 2.0\n    \n    # Attack 4: Forensics (Benford)\n    # *CRITICAL*: Add this column BEFORE creating duplicates, otherwise random values make rows unique!\n    df_attack[\"Expense\"] = np.random.randint(100, 999, size=len(df_attack))\n    \n    # Attack 3: Duplicates\n    # Append 20 duplicates AFTER all columns are set\n    df_attack = pd.concat([df_attack, df_attack.iloc[:20]])\n    \n    # Attack 5: Logic\n    df_attack.loc[df_attack.index[:5], \"bmi\"] = -10.0\n\n    # 5. EXECUTE SCAN\n    print(\"\\n--- 3. RUNNING OMNI SCAN ---\")\n    safe, report = titan.validate(\n        df_attack, \n        target_col=\"target\", \n        date_col=\"Date\", \n        consistency_rules=[\"bmi > 0\"], \n        subgroups=[\"Region\"]\n    )\n    \n    # 6. VERIFY RESULTS\n    print(f\"\\n[FINAL VERDICT]: {'âœ… SAFE' if safe else 'ðŸ›‘ UNSAFE'}\")\n    failures = report[\"traffic_light\"][report[\"traffic_light\"][\"Status\"] == \"RED\"]\n    \n    if not failures.empty:\n        print(\"\\n\" + failures.to_string(index=False))\n    \n    # 7. PROOF\n    print(\"\\n[OMNI PROOF]\")\n    reasons = str(failures[\"Reason\"].values)\n    features = str(failures[\"Feature\"].values)\n    \n    checks = {\n        \"Volume\": \"Volumetric\" in reasons,\n        \"Fairness\": \"Fairness\" in reasons,\n        \"Duplicates\": \"Duplicate\" in reasons,\n        \"Benford\": \"Benford\" in reasons or \"Expense\" in features,\n        \"Logic\": \"bmi > 0\" in reasons\n    }\n    \n    for name, result in checks.items():\n        print(f\"{name:<12} Detected? {'âœ… YES' if result else 'âŒ NO'}\")\n        \n    if all(checks.values()):\n        print(\"\\nðŸš€ TITAN OMNI: ALL 5 ATTACK VECTORS NEUTRALIZED. ZERO GAPS CONFIRMED.\")\n    else:\n        print(\"\\nâš ï¸ WARNING: Some attacks slipped through.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T18:31:31.587585Z","iopub.execute_input":"2025-12-28T18:31:31.587973Z","iopub.status.idle":"2025-12-28T18:31:32.103343Z","shell.execute_reply.started":"2025-12-28T18:31:31.587947Z","shell.execute_reply":"2025-12-28T18:31:32.102383Z"}},"outputs":[{"name":"stdout","text":"--- 1. INITIALIZING TITAN OMNI ---\n","output_type":"stream"},{"name":"stderr","text":"TVF Omni Online. Baseline: 442 rows.\n","output_type":"stream"},{"name":"stdout","text":"\n--- 2. GENERATING 'HYDRA' ATTACK DATA ---\n\n--- 3. RUNNING OMNI SCAN ---\n\n[FINAL VERDICT]: ðŸ›‘ UNSAFE\n\n              Module                                 Feature Status                                          Reason\nAlgorithmic Fairness Fairness Warning on 'Region' (Disparity    RED Fairness Warning on 'Region' (Disparity: 96.2%)\n      Business Logic                             Rule Failed    RED                            Rule Failed: bmi > 0\n    Causal Stability                                  Global    RED                                         age-bmi\n    Causal Stability                                  Global    RED                                          bmi-s1\n    Causal Stability                                  Global    RED                                          bmi-s2\n    Causal Stability                                  Global    RED                                          bmi-s4\n    Causal Stability                                  Global    RED                                          bmi-s5\n    Causal Stability                                  Global    RED                                      bmi-target\n           Forensics                                  Global    RED                              s4 (Benford Viol.)\n           Forensics                                  Global    RED                              s3 (Benford Viol.)\n           Forensics                                  Global    RED                              s2 (Benford Viol.)\n           Forensics                                  Global    RED                              s1 (Benford Viol.)\n           Forensics                                  Global    RED                              bp (Benford Viol.)\n           Forensics                                  Global    RED                             bmi (Benford Viol.)\n           Forensics                                  Global    RED                              s6 (Benford Viol.)\n           Forensics                                  Global    RED                             age (Benford Viol.)\n           Forensics                                  Global    RED                              s5 (Benford Viol.)\n           Forensics                                  Global    RED                             sex (Benford Viol.)\n  Multivariate Drift            Structure Shift (Error Ratio    RED            Structure Shift (Error Ratio: 6.56x)\n       Numeric Drift                                  Global    RED                                          target\n          Ops Health                                  Global    RED                          Duplicate Rows (10.2%)\n          Ops Health            Volumetric Drift (Size Ratio    RED            Volumetric Drift (Size Ratio: 0.45x)\n            Temporal                                  Global    RED                          Time Travel (Unsorted)\n\n[OMNI PROOF]\nVolume       Detected? âœ… YES\nFairness     Detected? âœ… YES\nDuplicates   Detected? âœ… YES\nBenford      Detected? âœ… YES\nLogic        Detected? âœ… YES\n\nðŸš€ TITAN OMNI: ALL 5 ATTACK VECTORS NEUTRALIZED. ZERO GAPS CONFIRMED.\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}