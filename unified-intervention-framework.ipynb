{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================================================\n# UNIFIED INTERVENTION FRAMEWORK (UIF) - PLATINUM EDITION\n# Role: Principal Causal AI Researcher & Simulation Architect\n# Version: 4.1 (Final Production Release - Fixed Validation)\n# ============================================================\n\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport networkx as nx\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nimport torch\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import KFold\nfrom typing import Dict, List, Optional, Any\n\n# GPU Configuration\nwarnings.filterwarnings(\"ignore\")\nUSE_GPU = torch.cuda.is_available()\nDEVICE = \"cuda\" if USE_GPU else \"cpu\"\nprint(f\"✅ UIF PLATINUM ONLINE | Device: {DEVICE}\")\n\n# ============================================================\n# 1. THE PLATINUM SIMULATOR ENGINE\n# ============================================================\nclass PlatinumCausalEngine:\n    def __init__(self, causal_graph, data: pd.DataFrame, verbose=True):\n        self.G = self._build_graph(causal_graph)\n        self.df = data.copy()\n        self.verbose = verbose\n        self.nodes = list(nx.topological_sort(self.G))\n        self.models = {}\n        self.model_types = {}\n        self.residuals = pd.DataFrame(index=self.df.index)\n        \n        if self.verbose: print(\"⚙️  FITTING ADAPTIVE MODELS...\")\n        self._fit_adaptive_models()\n        self._compute_residuals()\n        if self.verbose: print(\"    -> Models Fitted & Residuals Computed.\")\n\n    def _build_graph(self, g_input):\n        if isinstance(g_input, list):\n            G = nx.DiGraph()\n            G.add_edges_from(g_input)\n            return G\n        return g_input.copy()\n\n    def _fit_adaptive_models(self):\n        for node in self.nodes:\n            parents = list(self.G.predecessors(node))\n            if not parents:\n                self.residuals[node] = self.df[node] \n                continue\n\n            X = self.df[parents]\n            y = self.df[node]\n\n            lin_scores, xgb_scores = [], []\n            kf = KFold(n_splits=3, shuffle=True, random_state=42)\n\n            for train_idx, val_idx in kf.split(X):\n                X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n                y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n                \n                lin = LinearRegression().fit(X_train, y_train)\n                lin_scores.append(lin.score(X_val, y_val))\n                \n                xg = xgb.XGBRegressor(n_estimators=50, max_depth=4, device=DEVICE, enable_categorical=True)\n                xg.fit(X_train, y_train)\n                xgb_scores.append(xg.score(X_val, y_val))\n\n            avg_lin, avg_xgb = np.mean(lin_scores), np.mean(xgb_scores)\n\n            if avg_xgb > avg_lin + 0.05:\n                self.models[node] = xgb.XGBRegressor(n_estimators=100, max_depth=5, learning_rate=0.05, device=DEVICE)\n                self.models[node].fit(X, y)\n                self.model_types[node] = \"XGBoost (Non-Linear)\"\n            else:\n                self.models[node] = LinearRegression()\n                self.models[node].fit(X, y)\n                self.model_types[node] = \"Linear (Robust)\"\n            \n            if self.verbose:\n                print(f\"    - {node}: Selected {self.model_types[node]} (R2: {max(avg_lin, avg_xgb):.3f})\")\n\n    def _compute_residuals(self):\n        for node in self.nodes:\n            parents = list(self.G.predecessors(node))\n            if not parents: continue\n            X = self.df[parents]\n            pred = self.models[node].predict(X)\n            self.residuals[node] = self.df[node] - pred\n\n    def simulate_intervention(self, treatment: dict, target: str, n_boot=0) -> dict:\n        mu = self._run_simulation_pass(self.df, self.residuals, treatment, target)\n        \n        result = {\n            \"E_y_do\": mu,\n            \"std_error\": 0.0,\n            \"ci_lower\": mu, \"ci_upper\": mu,\n            \"model_used\": self.model_types.get(target, \"Direct\"),\n            \"treatment\": treatment\n        }\n\n        if n_boot > 0:\n            estimates = []\n            for _ in range(n_boot):\n                res_boot = self.residuals.sample(frac=1.0, replace=True)\n                df_boot = self.df.loc[res_boot.index].copy()\n                val = self._run_simulation_pass(df_boot, res_boot, treatment, target)\n                estimates.append(val)\n            \n            result[\"ci_lower\"] = np.percentile(estimates, 2.5)\n            result[\"ci_upper\"] = np.percentile(estimates, 97.5)\n            result[\"std_error\"] = np.std(estimates)\n            \n        return result\n\n    def _run_simulation_pass(self, df_base, res_base, treatment, target):\n        df_sim = df_base.copy()\n        for t_var, t_val in treatment.items():\n            df_sim[t_var] = t_val\n        for node in self.nodes:\n            if node in treatment: continue\n            parents = list(self.G.predecessors(node))\n            if not parents: continue\n            X = df_sim[parents]\n            base_val = self.models[node].predict(X)\n            df_sim[node] = base_val + res_base[node].values\n        return df_sim[target].mean()\n\n\n# ============================================================\n# 3. DEEP VALIDATION PROOF (Fixed for Simpson's Paradox)\n# ============================================================\ndef run_deep_validation():\n    print(\"\\n========================================================\")\n    print(\"RUNNING FINAL DEEP VALIDATION (PLATINUM EDITION)\")\n    print(\"========================================================\")\n    \n    # --- A. GENERATE SYNTHETIC WORLD (Ground Truth) ---\n    # We FORCE Simpson's Paradox:\n    # 1. Holiday causes HUGE increase in Price (+40).\n    # 2. Holiday causes HUGE increase in Sales (+200).\n    # 3. Price causes MODERATE decrease in Sales (-2.5).\n    # Result: Price and Sales will look POSITIVELY correlated (both high on holidays)\n    # despite the true causal link being negative.\n    \n    np.random.seed(2025)\n    n = 5000\n    Holiday = np.random.binomial(1, 0.4, n) # More holidays to balance classes\n    \n    # Mechanism: Holidays drive prices up massively\n    Price = 50 + 40*Holiday + np.random.normal(0, 5, n) \n    \n    # Mechanism: True causal effect of Price is -2.5\n    # But Holiday adds +200, overpowering the price drop\n    Sales = 200 - 2.5*Price + 200*Holiday + np.random.normal(0, 10, n) \n    \n    df = pd.DataFrame({\"Holiday\": Holiday, \"Price\": Price, \"Sales\": Sales})\n    edges = [(\"Holiday\", \"Price\"), (\"Holiday\", \"Sales\"), (\"Price\", \"Sales\")]\n    \n    # --- B. EXECUTE THE ENGINE ---\n    engine = PlatinumCausalEngine(edges, df)\n    \n    # --- C. EXTRAPOLATION TEST ---\n    print(\"\\n--- TEST 1: EXTRAPOLATION & UNCERTAINTY ---\")\n    res_base = engine.simulate_intervention({\"Price\": 50}, \"Sales\", n_boot=100)\n    res_high = engine.simulate_intervention({\"Price\": 90}, \"Sales\", n_boot=100)\n    \n    effect = res_high[\"E_y_do\"] - res_base[\"E_y_do\"]\n    true_effect = -2.5 * (90 - 50) # -100.0\n    \n    print(f\"Scenario: Increase Price from $50 to $90 (Delta +$40)\")\n    print(f\"Ground Truth Effect: {true_effect:.2f}\")\n    print(f\"Platinum Simulated:  {effect:.2f}\")\n    print(f\"Confidence Interval: [{effect - 1.96*res_high['std_error']:.2f}, {effect + 1.96*res_high['std_error']:.2f}]\")\n    \n    if abs(effect - true_effect) < 5.0:\n        print(\"✅ PASSED: Engine correctly extrapolated linear causal trend.\")\n    else:\n        print(\"❌ FAILED: Engine failed to extrapolate.\")\n\n    # --- D. SIMPSON'S PARADOX TEST ---\n    print(\"\\n--- TEST 2: SIMPSON'S PARADOX RESOLUTION ---\")\n    naive_corr = df[\"Price\"].corr(df[\"Sales\"])\n    causal_slope = effect / 40.0\n    \n    print(f\"Naive Correlation (P(Sales|Price)): {naive_corr:.3f} (Positively Correlated!)\")\n    print(f\"Causal Effect (P(Sales|do(Price))): {causal_slope:.3f} per unit (Negatively Causal!)\")\n    \n    if naive_corr > 0.3 and causal_slope < -2.0:\n        print(\"✅ PASSED: Engine correctly flipped the sign (Resolved Paradox).\")\n    else:\n        print(\"❌ FAILED: Engine failed to resolve paradox.\")\n\nif __name__ == \"__main__\":\n    run_deep_validation()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T20:30:24.875537Z","iopub.execute_input":"2025-12-28T20:30:24.876286Z","iopub.status.idle":"2025-12-28T20:30:25.698162Z","shell.execute_reply.started":"2025-12-28T20:30:24.876257Z","shell.execute_reply":"2025-12-28T20:30:25.697524Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}