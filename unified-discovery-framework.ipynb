{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==============================================================================\n# THE UNIFIED DISCOVERY FRAMEWORK (PRODUCTION EDITION)\n# ==============================================================================\n# AUTHOR: AI Assistant\n# VERSION: 3.0 (Final)\n#\n# PURPOSE:\n# This engine autonomously discovers causal drivers in messy economic/business data.\n# It solves 4 specific failure modes of traditional Data Science:\n# 1. Hidden Variables (Structure Scan)\n# 2. Spurious Proxies (Double ML / Non-Linear FWL)\n# 3. Regime Changes (Ruptures / PELT)\n# 4. Endogeneity (Instrumental Variable Scan)\n# ==============================================================================\n\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.metrics import r2_score\nfrom scipy.stats import pearsonr\nimport ruptures as rpt  # pip install ruptures\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n# ------------------------------------------------------------------------------\n# 1. THE ENGINE CLASS\n# ------------------------------------------------------------------------------\nclass UnifiedDiscoveryEngine:\n    def __init__(self, df, target_col, date_col='Date_Index'):\n        self.raw_df = df.copy()\n        self.target = target_col\n        self.date_col = date_col\n        self.known_features = [c for c in df.columns if c not in [target_col, date_col]]\n        print(f\"üöÄ ENGINE ONLINE. Target: '{self.target}'\")\n        print(f\"   Initial Knowledge Base: {self.known_features}\")\n\n    # PHASE 1: DISCOVERY & PHYSICS SCAN (Regions I & II)\n    def scan_environment(self):\n        print(\"\\n>> [PHASE 1] SCANNING ENVIRONMENT (Structure & Time)...\")\n        X = self.raw_df[self.known_features]\n        y = self.raw_df[self.target]\n        \n        # Baseline Model (XGBoost for non-linearity)\n        self.model = xgb.XGBRegressor(n_estimators=100, max_depth=3, random_state=42)\n        self.model.fit(X, y)\n        baseline_r2 = self.model.score(X, y)\n        print(f\"   Baseline Model R¬≤: {baseline_r2:.4f}\")\n        \n        # Residual Analysis (Mining the Unknown)\n        preds = self.model.predict(X)\n        residuals = y - preds\n        \n        # Temporal Scan: Check for Lags (Autocorrelation in Errors)\n        max_corr = 0\n        best_lag = 0\n        for l in [1, 2, 3, 7, 30]: \n            res_shift = np.roll(residuals, l)\n            if len(residuals) > l:\n                corr = pearsonr(residuals[l:], res_shift[l:])[0]\n                if abs(corr) > max_corr:\n                    max_corr = abs(corr)\n                    best_lag = l\n        \n        if max_corr > 0.15:\n            print(f\"   ‚ö†Ô∏è  TEMPORAL GAP: Lag detected (Corr: {max_corr:.2f}). Suggesting Lag-{best_lag}.\")\n            return f\"Sales_Lag_{best_lag}\"\n        \n        return None\n\n    def add_feature(self, name, data):\n        self.raw_df[name] = data\n        if name not in self.known_features:\n            self.known_features.append(name)\n        print(f\"   ‚ûï ADDED: '{name}'\")\n\n    # PHASE 2: CAUSAL DOMINANCE (Double ML / Non-Linear Proxy Kill)\n    def verify_causality_nonlinear(self):\n        print(\"\\n>> [PHASE 2] CAUSAL DOMINANCE (Non-Linear Proxy Kill)...\")\n        \n        # Anchors = Trusted Variables (History, Price, External)\n        anchors = [f for f in self.known_features if \"Lag\" in f or \"Price\" in f or \"Competitor\" in f]\n        suspects = [f for f in self.known_features if f not in anchors]\n        \n        if not anchors: \n            print(\"   (Skipping: No Anchors found to test against).\")\n            return\n\n        survivors = list(anchors)\n        \n        for suspect in suspects:\n            # DOUBLE ML CHECK:\n            # 1. Regress Suspect on Anchors (Is Suspect redundant?)\n            X_anchor = self.raw_df[anchors]\n            y_suspect = self.raw_df[[suspect]]\n            \n            m_check = RandomForestRegressor(n_estimators=50, max_depth=5, random_state=42)\n            m_check.fit(X_anchor, y_suspect.values.ravel())\n            pred_suspect = m_check.predict(X_anchor)\n            \n            explained_variance = r2_score(y_suspect, pred_suspect)\n            print(f\"   ‚öîÔ∏è  AUDITING '{suspect}'...\")\n            print(f\"       -> Redundancy Score (RF-R¬≤): {explained_variance:.4f}\")\n            \n            # 2. If Redundant (>90%), check Residuals (Does Suspect contain ANY unique info?)\n            if explained_variance > 0.90:\n                resid_suspect = y_suspect.values.ravel() - pred_suspect\n                \n                # Get Sales Residuals (Sales - Anchors)\n                m_sales = RandomForestRegressor(n_estimators=50, max_depth=5, random_state=42)\n                m_sales.fit(X_anchor, self.raw_df[self.target])\n                resid_sales = self.raw_df[self.target] - m_sales.predict(X_anchor)\n                \n                corr_resid = pearsonr(resid_sales, resid_suspect)[0]\n                print(f\"       -> Independent Signal (Resid Corr): {corr_resid:.4f}\")\n                \n                if abs(corr_resid) < 0.15:\n                    print(f\"       ‚ùå REJECTED: Pure Proxy. Removing.\")\n                    if suspect in self.known_features:\n                        self.known_features.remove(suspect)\n                else:\n                    print(f\"       ‚ö†Ô∏è  KEPT: High overlap, but unique signal exists.\")\n                    survivors.append(suspect)\n            else:\n                print(\"       ‚úÖ VERIFIED: Unique Driver.\")\n                survivors.append(suspect)\n        \n        self.known_features = list(set(self.known_features) & set(survivors + anchors))\n\n    # PHASE 3: REGIME CHANGE DETECTION (Ruptures / Drift)\n    def detect_regimes_and_drift(self):\n        print(\"\\n>> [PHASE 3] REGIME CHANGE DETECTION (Ruptures)...\")\n        \n        X = self.raw_df[self.known_features]\n        y = self.raw_df[self.target]\n        \n        # Use PELT Algorithm to find structural breaks in model error\n        try:\n            global_model = Ridge().fit(X, y)\n            residuals = (y - global_model.predict(X)).values.reshape(-1, 1)\n            algo = rpt.Pelt(model=\"rbf\").fit(residuals)\n            result = algo.predict(pen=10)\n        except:\n            print(\"   (Ruptures scan failed or no library, skipping)\")\n            result = []\n        \n        if len(result) > 1:\n            last_cp = result[-2]\n            print(f\"   üìç Detected Regime Change at Day {last_cp}\")\n            \n            # Compare Coefficients: Before vs After\n            df_past = self.raw_df.iloc[:last_cp]\n            df_recent = self.raw_df.iloc[last_cp:]\n            \n            if len(df_recent) < 20: \n                print(\"   (Recent regime too short for stability analysis)\")\n                return\n\n            m_past = LinearRegression().fit(df_past[self.known_features], df_past[self.target])\n            m_recent = LinearRegression().fit(df_recent[self.known_features], df_recent[self.target])\n            \n            print(f\"   {'Feature':<15} | {'Past':<8} | {'Recent':<8} | {'Status'}\")\n            print(\"-\" * 60)\n            \n            for i, feat in enumerate(self.known_features):\n                past = m_past.coef_[i]\n                recent = m_recent.coef_[i]\n                \n                # Magnitude Ratio Check\n                mag_past = abs(past)\n                mag_recent = abs(recent)\n                \n                if mag_past < 0.001: ratio = 1.0\n                else: ratio = mag_recent / mag_past\n                \n                if ratio < 0.2: status = \"üíÄ ZOMBIE\"\n                elif ratio < 0.5: status = \"‚ö†Ô∏è  DECAYING\"\n                elif ratio > 1.5: status = \"üî• SURGING\"\n                else: status = \"‚úÖ STABLE\"\n                \n                print(f\"   {feat:<15} | {past:<8.2f} | {recent:<8.2f} | {status}\")\n        else:\n            print(\"   ‚úÖ No Regime Change Detected (Stable World).\")\n\n    # PHASE 4: IV DISCOVERY (Endogeneity Fix)\n    def scan_for_instruments(self, potential_instruments):\n        print(\"\\n>> [PHASE 4] IV DISCOVERY (Endogeneity Scan)...\")\n        \n        # 1. Reduced Form: Regress Sales on History ONLY (No Price)\n        # This isolates \"Sales Variance unexplained by Momentum\"\n        exog_controls = [f for f in self.known_features if \"Lag\" in f]\n        if not exog_controls:\n            print(\"   (No Lags found. Cannot perform Reduced Form check.)\")\n            return\n            \n        X_reduced = self.raw_df[exog_controls]\n        y = self.raw_df[self.target]\n        m_red = LinearRegression().fit(X_reduced, y)\n        resid_reduced = y - m_red.predict(X_reduced)\n        \n        if 'My_Price' not in self.known_features: return\n        price = self.raw_df['My_Price']\n        \n        print(f\"   {'Candidate':<15} | {'Corr(Price)':<12} | {'Corr(Resid)':<12} | {'Verdict'}\")\n        print(\"-\" * 65)\n        \n        for name, data in potential_instruments.items():\n            # Condition 1: Strong First Stage (Must move Price)\n            corr_price = pearsonr(data, price)[0]\n            \n            # Condition 2: Exclusion Restriction Check\n            # A valid IV affects Sales *only* via Price.\n            # So Corr(IV, Sales_Resid) should be roughly Corr(IV, Price) * Beta_Price.\n            # If it's too high, it means IV affects Sales directly (Invalid).\n            # If it's too low, it means Price doesn't drive Sales (Weak Model).\n            # We look for a \"Clean Signal\" (High Price Corr, Moderate/Low Resid Corr).\n            \n            corr_resid = pearsonr(data, resid_reduced)[0]\n            \n            status = \"‚ùå\"\n            if abs(corr_price) > 0.3:\n                # Heuristic: Resid correlation should be weaker than Price correlation\n                if abs(corr_resid) < abs(corr_price): \n                    status = \"‚úÖ VALID IV\"\n            \n            print(f\"   {name:<15} | {corr_price:<12.2f} | {corr_resid:<12.2f} | {status}\")\n\n# ------------------------------------------------------------------------------\n# 2. THE BOSS LEVEL SIMULATION (VERIFICATION)\n# ------------------------------------------------------------------------------\ndef run_boss_level():\n    print(\"\\n=====================================================================\")\n    print(\"  RUNNING BOSS LEVEL SIMULATION (4 TRAPS ACTIVE)\")\n    print(\"=====================================================================\")\n    np.random.seed(2025)\n    n = 1000\n    t = np.arange(n)\n    \n    # 1. Endogeneity (IV = Cost of Goods)\n    cost_of_goods = np.random.normal(50, 5, n) \n    demand_shock = np.random.normal(0, 10, n)\n    my_price = 1.5 * cost_of_goods + 0.5 * demand_shock + np.random.normal(0, 2, n)\n    \n    # 2. Dynamics\n    sales = np.zeros(n)\n    ad_spend = np.zeros(n)\n    email_spend = np.random.normal(20, 5, n)\n    email_eff = np.where(t < 700, 3.0, 0.0) # Dies at 700\n    \n    for i in range(1, n):\n        # Sales Physics\n        base = 2000 - 4 * my_price[i] + (email_spend[i] * email_eff[i]) + demand_shock[i]\n        sales[i] = base + 0.4 * sales[i-1] + np.random.normal(0, 5)\n        \n        # TRAP: Ad Spend is Non-Linear Proxy\n        ad_spend[i] = (sales[i-1] / 100) ** 2 \n\n    df = pd.DataFrame({\n        'Date_Index': t,\n        'My_Price': my_price,\n        'Ad_Spend': ad_spend,\n        'Email_Spend': email_spend,\n        'Sales': sales,\n        'Sales_Lag_1': np.roll(sales, 1)\n    }).iloc[10:]\n    \n    # --- RUN ENGINE ---\n    engine = UnifiedDiscoveryEngine(df, 'Sales')\n    \n    # Add Features (Simulating Data Ingestion)\n    engine.add_feature('My_Price', my_price[10:])\n    engine.add_feature('Ad_Spend', ad_spend[10:])\n    engine.add_feature('Email_Spend', email_spend[10:])\n    engine.add_feature('Sales_Lag_1', df['Sales_Lag_1'])\n    \n    # Execute Pipeline\n    engine.scan_environment()          # Should suggest Lags\n    engine.verify_causality_nonlinear() # Should kill Ad_Spend\n    engine.detect_regimes_and_drift()   # Should find Day 700 & Zombie Email\n    \n    # IV Scan\n    engine.scan_for_instruments({'Cost_Goods': cost_of_goods[10:]})\n\n# ------------------------------------------------------------------------------\n# 3. REAL WORLD DATA CONNECTOR (FRED / BLS / CENSUS)\n# ------------------------------------------------------------------------------\ndef run_real_world_test(api_key=None):\n    if not api_key:\n        print(\"\\n(Skipping Real World Test. Add your FRED API Key to run.)\")\n        return\n\n    try:\n        import pandas_datareader as pdr\n        print(\"\\n=====================================================================\")\n        print(\"  RUNNING REAL WORLD TEST (FRED DATA)\")\n        print(\"=====================================================================\")\n        \n        # Download 3 Major Series\n        # 1. GDP (Gross Domestic Product)\n        # 2. UNRATE (Unemployment Rate)\n        # 3. CPIAUCSL (Inflation / CPI)\n        print(\">> DOWNLOADING DATA...\")\n        df = pdr.get_data_fred(['GDP', 'UNRATE', 'CPIAUCSL'], start='2000-01-01', end='2024-01-01', api_key=api_key)\n        \n        # Data Prep: Calculate % Growth to handle non-stationarity\n        df = df.pct_change().dropna() \n        df['Date_Index'] = np.arange(len(df))\n        \n        # Run Engine\n        engine = UnifiedDiscoveryEngine(df, 'GDP')\n        engine.add_feature('Unemployment', df['UNRATE'])\n        engine.add_feature('Inflation', df['CPIAUCSL'])\n        \n        engine.scan_environment()\n        engine.verify_causality_nonlinear()\n        engine.detect_regimes_and_drift() # Did the rules change after 2020 (Covid)?\n        \n    except Exception as e:\n        print(f\"Error fetching real data: {e}\")\n\n# ==============================================================================\n# EXECUTION\n# ==============================================================================\nif __name__ == \"__main__\":\n    run_boss_level()\n    # run_real_world_test('FRED API KEY')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T19:36:50.492546Z","iopub.execute_input":"2025-12-26T19:36:50.492866Z","iopub.status.idle":"2025-12-26T19:36:52.311632Z","shell.execute_reply.started":"2025-12-26T19:36:50.492845Z","shell.execute_reply":"2025-12-26T19:36:52.310905Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}