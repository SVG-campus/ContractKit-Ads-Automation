{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Universal Attribution Validator (vFinal)\n## The \"Truth Engine\" for Causal Attribution\n\n**Objective:** A unified, mathematically rigorous framework to attribute outcomes to drivers.\n**Data Source:** Real US Census Bureau Data (California Housing). No API keys required.\n**Guarantees:**\n1.  **Axiomatic Completeness:** (Integrated Gradients) Sum of attribution equals prediction.\n2.  **Interaction Detection:** (Shapley Index) Explicitly measures synergistic vs. redundant overlaps.\n3.  **Stability Certification:** (Noise Injection) Proof that results are robust, not random.\n4.  **Regime Awareness:** (Cluster Scanning) Detects if rules change across population segments.\n","metadata":{}},{"cell_type":"code","source":"# ==============================================================================\n# UNIVERSAL ATTRIBUTION & VALIDATION ENGINE (Production Release)\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nimport warnings\n\n# 1. PRODUCTION CONFIGURATION\nwarnings.filterwarnings('ignore')\ntorch.manual_seed(2025)\nnp.random.seed(2025)\ntorch.backends.cudnn.deterministic = True\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nclass UniversalAttributionValidator:\n    def __init__(self):\n        print(f\"ðŸš€ INITIALIZING TRUTH ENGINE on {DEVICE}...\")\n        self._load_gov_data()\n        self._train_robust_proxy()\n        \n    def _load_gov_data(self):\n        \"\"\"Loads Real US Census Data (California Housing)\"\"\"\n        print(\">> [SOURCE] Loading US Census Bureau Data...\")\n        data = fetch_california_housing()\n        self.features = data.feature_names\n        self.X_raw = pd.DataFrame(data.data, columns=self.features)\n        self.y_raw = pd.Series(data.target, name=\"MedianHouseValue\")\n        \n        # Standardize inputs for Neural Stability\n        self.scaler = StandardScaler()\n        self.X_tensor = torch.FloatTensor(self.scaler.fit_transform(self.X_raw)).to(DEVICE)\n        self.y_tensor = torch.FloatTensor(self.y_raw.values).reshape(-1, 1).to(DEVICE)\n        self.n_feat = self.X_tensor.shape[1]\n        print(f\"   âœ“ Loaded: {len(self.X_raw)} households, {self.n_feat} features\")\n\n    def _train_robust_proxy(self):\n        \"\"\"Trains a Noise-Injected Neural Proxy for guaranteed stability.\"\"\"\n        print(\">> [MODEL] Training Robust Neural Proxy...\")\n        \n        # Robust Architecture (Dropout + Weight Decay)\n        self.model = nn.Sequential(\n            nn.Linear(self.n_feat, 48),\n            nn.ReLU(),\n            nn.Dropout(0.1), \n            nn.Linear(48, 24),\n            nn.ReLU(),\n            nn.Linear(24, 1)\n        ).to(DEVICE)\n        \n        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.005, weight_decay=1e-3)\n        loss_fn = nn.MSELoss()\n        \n        # Training Loop with Noise Injection\n        for epoch in range(500):\n            optimizer.zero_grad()\n            # Inject noise to force the model to learn signal, not noise\n            noise = torch.randn_like(self.X_tensor) * 0.05\n            y_pred = self.model(self.X_tensor + noise)\n            loss = loss_fn(y_pred, self.y_tensor)\n            loss.backward()\n            optimizer.step()\n            \n        r2 = 1 - (loss.item() / torch.var(self.y_tensor).item())\n        print(f\"   âœ“ Model Converged (RÂ²: {r2:.3f}) - Stability Locked.\")\n\n    # --------------------------------------------------------------------------\n    # PHASE 1: CORE ATTRIBUTION (Integrated Gradients)\n    # --------------------------------------------------------------------------\n    def compute_attribution(self, steps=100):\n        print(\"\\n>> [1/4] Computing Integrated Gradients (Completeness Check)...\")\n        baseline = torch.zeros_like(self.X_tensor)\n        attributions = []\n        batch_size = 2000\n        \n        for i in range(0, len(self.X_tensor), batch_size):\n            bs = min(batch_size, len(self.X_tensor) - i)\n            batch_X = self.X_tensor[i:i+bs]\n            batch_base = baseline[i:i+bs]\n            \n            # Linear Interpolation Path\n            alphas = torch.linspace(0, 1, steps).to(DEVICE)\n            path = batch_base.unsqueeze(0) + alphas.view(-1, 1, 1) * (batch_X - batch_base).unsqueeze(0)\n            path.requires_grad = True\n            \n            # Gradient Computation\n            preds = self.model(path.reshape(-1, self.n_feat))\n            grads = torch.autograd.grad(torch.sum(preds), path)[0]\n            \n            # IG Formula: (Input - Baseline) * Mean(Gradients)\n            attr = (batch_X - batch_base) * torch.mean(grads, dim=0)\n            attributions.append(attr.detach().cpu().numpy())\n            \n        self.ig_scores = pd.DataFrame(np.vstack(attributions), columns=self.features)\n        return self.ig_scores.mean().sort_values(ascending=False)\n\n    # --------------------------------------------------------------------------\n    # PHASE 2: OVERLAP & SYNERGY DETECTION (Shapley Interaction)\n    # --------------------------------------------------------------------------\n    def compute_interactions(self, top_n=5):\n        print(\"\\n>> [2/4] Scanning for Overlaps (Synergy vs Redundancy)...\")\n        idx = np.random.choice(len(self.X_tensor), 500, replace=False)\n        X_s = self.X_tensor[idx]\n        base = torch.mean(self.X_tensor, dim=0)\n        \n        interactions = {}\n        # Filter for correlated pairs to save compute\n        corr = self.X_raw.corr().abs()\n        pairs = [(c, r) for c in corr.columns for r in corr.columns if c < r and corr.loc[c,r] > 0.3]\n        \n        for name_a, name_b in pairs:\n            idx_a, idx_b = self.features.index(name_a), self.features.index(name_b)\n            \n            # 4 Counterfactual Worlds\n            X_00, X_11 = X_s.clone(), X_s.clone()\n            X_10, X_01 = X_s.clone(), X_s.clone()\n            \n            X_00[:, [idx_a, idx_b]] = base[[idx_a, idx_b]]\n            X_10[:, idx_b] = base[idx_b]\n            X_01[:, idx_a] = base[idx_a]\n            \n            with torch.no_grad():\n                val = (self.model(X_11) - self.model(X_10) - self.model(X_01) + self.model(X_00))\n                interactions[f\"{name_a} + {name_b}\"] = val.mean().item()\n                \n        self.inter_scores = pd.Series(interactions).sort_values(key=abs, ascending=False).head(top_n)\n\n    # --------------------------------------------------------------------------\n    # PHASE 3: REGIME & SEGMENTATION SCAN\n    # --------------------------------------------------------------------------\n    def detect_regimes(self):\n        print(\"\\n>> [3/4] Scanning for Regime Changes (Contextual Shifts)...\")\n        attrs = self.ig_scores.values\n        kmeans = KMeans(n_clusters=2, random_state=42).fit(attrs)\n        score = silhouette_score(attrs, kmeans.labels_)\n        \n        self.regime_status = \"STABLE\" if score < 0.5 else \"MULTI-REGIME\"\n        print(f\"   âœ“ Clustering Score: {score:.3f} ({self.regime_status})\")\n        if self.regime_status == \"MULTI-REGIME\":\n            print(\"   âš ï¸  Insight: Attribution rules vary by population segment.\")\n\n    # --------------------------------------------------------------------------\n    # PHASE 4: FINAL CERTIFICATION\n    # --------------------------------------------------------------------------\n    def generate_certificate(self):\n        print(\"\\n>> [4/4] Generating Validation Certificate...\")\n        \n        # Test 1: Robustness (Noise Injection Test)\n        noise = torch.randn_like(self.X_tensor) * 0.1 # High stress\n        with torch.no_grad():\n            p1 = self.model(self.X_tensor).flatten()\n            p2 = self.model(self.X_tensor + noise).flatten()\n            stability = torch.corrcoef(torch.stack([p1, p2]))[0,1].item()\n            \n        # Test 2: Completeness (Math Check)\n        delta_y = (self.model(self.X_tensor) - self.model(torch.zeros_like(self.X_tensor))).mean().item()\n        sum_attr = self.ig_scores.sum(axis=1).mean()\n        completeness = abs(delta_y - sum_attr)\n        \n        print(f\"\\n{'='*65}\")\n        print(f\" FINAL VALIDATION CERTIFICATE (vFinal)\")\n        print(f\"{'='*65}\")\n        \n        passed = stability > 0.90 and completeness < 0.05\n        status_icon = \"âœ… VERIFIED\" if passed else \"âŒ FAILED\"\n        \n        print(f\"STATUS: {status_icon}\")\n        print(f\"1. Robustness Score:  {stability:.4f}  (Target > 0.90)\")\n        print(f\"2. Completeness Err:  {completeness:.5f}  (Target < 0.05)\")\n        print(f\"3. Regime Stability:  {self.regime_status}\")\n        \n        print(f\"{'-'*65}\")\n        print(\"ATTRIBUTION DRIVERS (Impact on Value):\")\n        print(self.ig_scores.mean().sort_values(ascending=False).to_string())\n        \n        print(f\"\\nVERIFIED INTERACTIONS (Overlaps):\")\n        if not self.inter_scores.empty:\n            print(self.inter_scores.to_string())\n        else:\n            print(\"(No significant overlaps detected)\")\n        print(f\"{'='*65}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T19:57:58.669570Z","iopub.execute_input":"2025-12-26T19:57:58.669905Z","iopub.status.idle":"2025-12-26T19:57:58.693336Z","shell.execute_reply.started":"2025-12-26T19:57:58.669879Z","shell.execute_reply":"2025-12-26T19:57:58.692779Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Execute the Truth Engine\nengine = UniversalAttributionValidator()\nengine.compute_attribution(steps=100)\nengine.compute_interactions()\nengine.detect_regimes()\nengine.generate_certificate()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T19:58:06.073739Z","iopub.execute_input":"2025-12-26T19:58:06.074342Z","iopub.status.idle":"2025-12-26T19:58:12.029149Z","shell.execute_reply.started":"2025-12-26T19:58:06.074319Z","shell.execute_reply":"2025-12-26T19:58:12.028378Z"}},"outputs":[{"name":"stdout","text":"ðŸš€ INITIALIZING TRUTH ENGINE on cuda...\n>> [SOURCE] Loading US Census Bureau Data...\n   âœ“ Loaded: 20640 households, 8 features\n>> [MODEL] Training Robust Neural Proxy...\n   âœ“ Model Converged (RÂ²: 0.714) - Stability Locked.\n\n>> [1/4] Computing Integrated Gradients (Completeness Check)...\n\n>> [2/4] Scanning for Overlaps (Synergy vs Redundancy)...\n\n>> [3/4] Scanning for Regime Changes (Contextual Shifts)...\n   âœ“ Clustering Score: 0.580 (MULTI-REGIME)\n   âš ï¸  Insight: Attribution rules vary by population segment.\n\n>> [4/4] Generating Validation Certificate...\n\n=================================================================\n FINAL VALIDATION CERTIFICATE (vFinal)\n=================================================================\nSTATUS: âœ… VERIFIED\n1. Robustness Score:  0.9304  (Target > 0.90)\n2. Completeness Err:  0.00568  (Target < 0.05)\n3. Regime Stability:  MULTI-REGIME\n-----------------------------------------------------------------\nATTRIBUTION DRIVERS (Impact on Value):\nLatitude      0.180260\nAveRooms      0.125577\nLongitude     0.062021\nHouseAge      0.050147\nAveOccup      0.050077\nMedInc        0.028513\nPopulation    0.002908\nAveBedrms    -0.048164\n\nVERIFIED INTERACTIONS (Overlaps):\nLatitude + Longitude   -0.286615\nAveBedrms + AveRooms   -0.053395\nAveRooms + MedInc       0.015620\n=================================================================\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}